{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <img style=\"float:right;\" src=\"images/snext-logo.png\"/>\n",
    "    <div style=\"float:left;color:#626262;padding-top:30px\"><h1>Exercise: Unsupervised learning in Python with scikit-learn</h1></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebooks contains the skeleton of a simple data analytics documentation that demonstrates two unsupervised algorithms on a given dataset.\n",
    "\n",
    "Walk through the analysis be executing the cells one by one, complete the contained assignments then apply the learnings to a new case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Case description\n",
    "\n",
    "### Business Problem\n",
    "In financial institutions, the process of loan approval and determining the interest rate offered is critical in mitigating potential risks and maximizing returns. The decision-making process involves assessing the risk of each loan application based on various factors, such as credit score, income, and past repayment history. The assessment informs the bank of the probability of the borrower defaulting on the loan, which affects the interest rate offered to the applicant. Therefore, having a reliable and accurate risk assessment model is essential for financial institutions to make informed decisions.\n",
    "\n",
    "### Research Problem\n",
    "The research problem is to develop a classification model that can accurately classify loan applications into risk or no-risk categories. The model will review historical data on past loan applications and outcomes to identify patterns and predict the probability of the loan defaulting. Based on the model output, the loan applications shall be classified into those with low or high risk. The outcome of the model will help the bank make informed decisions on the loan amount, interest rates, and payment schedules, thus mitigating potential risks and enhancing returns.\n",
    "\n",
    "### Training Data\n",
    "The training dataset will consist of past loan applications and the corresponding outcomes. The data points collected will include the borrower's credit score, income, years of experience, and financial history such as investments, credit card debt, mortgage information, and other assets. The outcome variable will be a binary classification of either a loan default or no default. The model will undergo a series of tests using cross-validation techniques before implementation.\n",
    "\n",
    "### Exercise\n",
    "Developing a risk assessment model for loan applications is crucial for financial institutions to minimize risks and maximize profitability. Students in a university can engage in this problem to gain hands-on experience with data analysis and predictive modeling. The project will involve building and benchmarking classification models that can accurately classify loan applications, considering various features to predict if a loan is likely to default or not. The project will allow students to learn the methods of data pre-processing, model building and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Data loading, preparation and exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load required libraries and jupyter extenions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# load Jupyter plugins to enable SQL to query data and display plots inline (below the code cell)\n",
    "\n",
    "%load_ext sql\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data for this exercise is contained in a sqlite database that is compressed with ZIP\n",
    "# ZIP file is expected to be in folder data \n",
    "\n",
    "# Uncompress database / skip this if you downloaded and unzipped the database manually\n",
    "import zipfile\n",
    "zipfile.ZipFile('data/snext-data.zip', 'r').extractall('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql sqlite:///data/snext-database.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = %sql SELECT * FROM credit_ger\n",
    "df = data.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set index, shorten/unify feature names\n",
    "df = df.set_index([\"id\"])\n",
    "df = df.rename({\n",
    "    \"Age\": \"age\",\n",
    "    \"Sex\": \"sex\",\n",
    "    \"Job\": \"job\",\n",
    "    \"Housing\": \"housing\",\n",
    "    \"Saving_accounts\": \"savings\", \n",
    "    \"Checking_account\": \"cash\",\n",
    "    \"Credit_amount\": \"amount\",\n",
    "    \"Duration\": \"duration\",\n",
    "    \"Purpose\": \"purpose\",\n",
    "    \"Risk\": \"risk\"\n",
    "}, axis=\"columns\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Data preparation\n",
    "\n",
    "We'll apply the same transformations as in the chapter \"Feature Engineering\" from the notebook about supervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X will hold the input features (input for the model)\n",
    "# y will hold the label (the desired output of the model)\n",
    "X = pd.DataFrame()\n",
    "y = pd.DataFrame()\n",
    "\n",
    "# Dummy encode binary nominal features\n",
    "y[\"risk\"] = (df.risk == \"bad\")*1   # *1 translates True/False to 0/1\n",
    "X[\"male\"] = (df.sex == \"male\")*1\n",
    "\n",
    "# Pne-hot-encoding to nominal features with multiple categories\n",
    "\n",
    "df.purpose = df.purpose.str.slice(0,8) # shorten purpose string\n",
    "encoded_features = pd.get_dummies(df[[\"housing\",\"purpose\",\"savings\",\"cash\"]])\n",
    "X = pd.concat([X, encoded_features], axis=1) # append features to dataframe X with training data\n",
    "\n",
    "# all metric variables can remain as is, so we append them to the training data \n",
    "X = pd.concat([X,df[[\"age\", \"amount\", \"duration\"]]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 K-Means\n",
    "\n",
    "#### Determine the optimal number of clusters to generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inertias = []   # list contains the distances of the last merged clusters for each solution we plan to generate\n",
    "\n",
    "for n_clusters in range(2,10):  # generate multiple solutions for 2-9 clusters\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=10).fit(X)  # for each solution try 10 different random starting configurations for cluster centroids\n",
    "    inertias.append(kmeans.inertia_)  # add the inertia (distance of last merged clusters) to our list\n",
    "\n",
    "# Plot inertias \n",
    "plt.figure(figsize=(10,5))   \n",
    "plt.title('Elbow criterion')\n",
    "plt.xlabel(\"n_clusters\")\n",
    "plt.ylabel(\"inertia\")\n",
    "plt.plot(range(2,10), inertias, marker=\"o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "### <span style=\"color:#46B7E9;\">Assignment: Interpret the elbow criterion diagram</span>\n",
    "1. Think about what this line tells you about the generated cluster configurations. If unsure, rewatch the video about k-means.\n",
    "2. Apply the elbow criterion: What cluster configurations seems best? Determine the optimal number of clusters and put it in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_n_clusters = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=optimal_n_clusters, n_init=10)\n",
    "clusters = kmeans.fit_predict(X)  # apply clustering and assign each data row to the resulting cluster\n",
    "df[\"clusters\"] = clusters # add this to the original dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### <span style=\"color:#46B7E9;\">Assignment: Explore the clustering solution</span>\n",
    "1. Generate some descriptive statistics or plots to understand what the clusters represent\n",
    "2. Try to find fitting labels for all clusters like \"car loans for young people\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cluster Labels\n",
    "- Cluster 1: ...\n",
    "- Cluster 2: ...\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hints - butc try it yourself first, you can do it :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# some diagrams to get you started...\n",
    "\n",
    "import seaborn as sns\n",
    "fig, ax  = plt.subplots(2,3,figsize=(20,12))\n",
    "fig.suptitle(\"Interpretation Cluster nach Clusterzentroiden\")\n",
    "sns.scatterplot(x=df.duration, y = df.amount, hue=clusters, ax=ax[0,0], palette=\"bright\")\n",
    "sns.scatterplot(x=df.age, y = df.amount, hue=clusters, ax=ax[0,1], palette='bright')\n",
    "sns.scatterplot(x=df.age, y = df.duration, hue=clusters, ax=ax[0,2], palette='bright')\n",
    "\n",
    "ax[1,0].set_title (\"Altersverteilung in Clustern\")\n",
    "sns.boxplot(data=[ df[df.clusters == i].age for i in range(0,optimal_n_clusters)], ax=ax[1,0])\n",
    "\n",
    "ax[1,1].set_title (\"Kreditsummenverteilung in Clustern\")\n",
    "sns.boxplot(data=[ df[df.clusters == i].amount for i in range(0,optimal_n_clusters)], ax=ax[1,1])\n",
    "\n",
    "ax[1,2].set_title (\"Kreditdauerverteilung in Clustern\")\n",
    "sns.boxplot(data=[ df[df.clusters == i].duration for i in range(0,optimal_n_clusters)], ax=ax[1,2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Principal Component Analyis (PCA)\n",
    "In this section we use PCA to reduce the dimensions (number of variables) to make the dataset more handy and break it down so we can visualize all points in a 3D space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#46B7E9;\">Assignment: Before we get started...</span>\n",
    "1. Think about, why our degree of data preprocession is insufficient for the PCA. If unsure, rewatch the video an pay attention to the prerequisites.\n",
    "2. What is missing? Compare your thoughts to the transformation in the next (hidden) cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Further required processing of the data\n",
    "# Solution: we need to z-standardize the metric values, so the absolute value has no impact on the pca solution\n",
    "\n",
    "to_be_rescaled = [\"age\", \"amount\", \"duration\"]\n",
    "scaler = StandardScaler()   # tool from scikit learn library that applies z-standardization (mean of 0, std of 1)\n",
    "\n",
    "scaled_X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine the optimal number of components with a scree plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=5)  # extract five components from the data\n",
    "pca.fit (scaled_X)\n",
    "X_trans = pca.transform(scaled_X)  # apply pca to dataset to calculate coordinations in PCA space for each credit application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is how much variance each component can explain\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's plot this in a scree plot\n",
    "PC_values = range(0,pca.n_components_) # components\n",
    "plt.plot(PC_values, pca.explained_variance_ratio_, 'o-', linewidth=2, color='blue')  # plot % of variance explained vs. components\n",
    "plt.title('Scree Plot')\n",
    "plt.xlabel('Principal Component #')\n",
    "plt.ylabel('Variance Explained')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### <span style=\"color:#46B7E9;\">Assignment: Intepret the scree plot</span>\n",
    "1. Think about that the diagram explains to you\n",
    "2. How much of the original variance can the PCA reproduce approximately with 3 components (that we could visualize in a 3D diagram)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# calculate the answer\n",
    "sum(pca.explained_variance_ratio_[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "### <span style=\"color:#46B7E9;\">Assignment: Think about the shapes of the transformed dataset</span>\n",
    "What shape should the pca_data dataframe have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trans.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's explore the components by generating the loading matrix\n",
    "The matrix explains how the componants are assembled from the input variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading Matrix\")\n",
    "loading_matrix = pd.DataFrame(pca.components_.T, index=X.columns)\n",
    "loading_matrix.sort_values(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "### <span style=\"color:#46B7E9;\">Assignment: Interpret and name the components 0, 1 and 2</span>\n",
    "1. Sort the loading matrix by columns 0,1,2 and explore the top positive and negative input variables\n",
    "2. How would you label the components 0,1,2 given which and how the input variable weights from the loading matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Component 0:\n",
    "- Component 1:\n",
    "- Component 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's visualize the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mpl_toolkits.mplot3d \n",
    "plt.style.use('default')\n",
    " \n",
    "# Prepare 3D graph\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = plt.axes(projection='3d')\n",
    " \n",
    "# Plot scaled features\n",
    "xdata = X_trans[:,0]\n",
    "ydata = X_trans[:,1]\n",
    "zdata = X_trans[:,2]\n",
    " \n",
    "# Plot 3D plot\n",
    "ax.scatter3D(xdata, ydata, zdata, c=y, cmap='RdBu')  # c=y sets the color of the dot to the risk/no-risk variable we stored in dataframe y\n",
    " \n",
    "# Plot title of graph\n",
    "plt.title(\"3D Scatter of Credit Applications\")\n",
    "\n",
    "# Rotate diagram\n",
    "ax.view_init(30, 30, 0)\n",
    "\n",
    "# Plot x, y, z labels\n",
    "ax.set_xlabel('Component 0', rotation=150)\n",
    "ax.set_ylabel('Component 1')\n",
    "ax.set_zlabel('Component 2', rotation=60)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "### <span style=\"color:#46B7E9;\">Assignment: Analyze and interpret the plot</span>\n",
    "1. Try to change the orientation of the diagram (ax.view_init) to better see the risk high/low dot clouds. Is it possible / helpful?\n",
    "2. Write the code to describe the risk/no-risk credits using the components to check your visual interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# hint 1\n",
    "df_tmp = pd.concat([pd.DataFrame(X_trans),y], axis=1)\n",
    "df_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# hint 2\n",
    "df_tmp[df_tmp.risk==0].describe()\n",
    "# ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
